{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "TransfertLearningWikiV2.5.1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#   This software component is licensed by ST under BSD-3-Clause license,\n",
        "#   the \"License\"; You may not use this file except in compliance with the\n",
        "#   License. You may obtain a copy of the License at:\n",
        "#             https://opensource.org/licenses/BSD-3-Clause"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/STMicroelectronics/stm32ai/blob/master/AI_resources/VISION/transfer_learning/TransferLearning.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning on MobileNet\n",
        "\n",
        "This notebook shows the process of training a deep learning model via transfer learning on a custom dataset as well as the quantization of this model to in order to use it on a STM32 thanks to [STM32Cube.AI](https://www.st.com/en/embedded-software/x-cube-ai.html).\n",
        "\n",
        "This notebook is related to this ST Wiki [article](https://wiki.st.com/stm32mcu/wiki/AI:How_to_use_transfer_learning_to_perform_image_classification_on_STM32) about transfer learning.\n",
        "\n",
        "## Import the dataset\n",
        "\n",
        "The dataset is a directory containing one sub-directory per category, with images inside. For instance: \n",
        "```\n",
        "Dataset\n",
        "    ├── cats\n",
        "    │   ├── cat0001.jpg\n",
        "    │   ├── cat0002.jpg\n",
        "    │   └── ...\n",
        "    ├── dogs\n",
        "    └── horses\n",
        "```\n",
        "If you're using this in notebook in Colab, you should import your dataset a `Dataset.zip` file and a `Testset.zip`. It will be extracted later on. Also on Colab, make sure you choose the GPU runtime for faster training.\n",
        "\n",
        "You can also choose to use a dataset from the Tensorflow dataset repository.\n",
        "\n",
        "## Import necessary packages\n",
        "Imgaug needs to be manualy imported to ensure the usage of the latest version."
      ],
      "metadata": {
        "id": "WK4_53GdxscH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install imgaug --upgrade"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vzHlOMrRxnG",
        "outputId": "df6505ae-b389-42bc-82a8-9c88acd18208"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sklearn.metrics as sk_metrics\n",
        "import seaborn as sns\n",
        "import pathlib\n",
        "\n",
        "from imgaug import augmenters as iaa\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import datasets\n",
        "from random import randint\n",
        "import tensorflow_datasets as tfds"
      ],
      "outputs": [],
      "metadata": {
        "id": "f0_ifqaKGj41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model selection\n",
        "You can choose between a MobileNet V1 and V2."
      ],
      "metadata": {
        "id": "ZDQ3juJeyZ5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "MODEL_VERSION = 'V2' #@param [\"V1\", \"V2\"]\n",
        "IMG_SIZE = [128, 128]\n",
        "IMG_SIZE_TUPLE = (IMG_SIZE[0], IMG_SIZE[1])\n",
        "BATCH_SIZE = 16"
      ],
      "outputs": [],
      "metadata": {
        "id": "nYIqPjGuGyMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset selection\n",
        "Chose a dataset from the tensorflow dataset zoo or select `custom` to use your own dataset, if you do, you must also provide a test dataset."
      ],
      "metadata": {
        "id": "hxhVyVldKsc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "DATASET_NAME = 'rock_paper_scissors' #@param [\"custom\", \"tf_flowers\", \"uc_merced\", \"rock_paper_scissors\", \"imagenette/full-size\", \"horses_or_humans\"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "BmD0EYcNHU93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you chose a custom dataset, uncomment those lines to extract them."
      ],
      "metadata": {
        "id": "Wa8MPfTRyz0n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# !unzip -q Dataset.zip\n",
        "# !unzip -q Testset.zip"
      ],
      "outputs": [],
      "metadata": {
        "id": "svYI5JZgz9zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset import"
      ],
      "metadata": {
        "id": "EFjvSlSw0ZCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if DATASET_NAME == 'custom':\n",
        "  dataset = image_dataset_from_directory('Dataset',\n",
        "          shuffle=True,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          image_size=IMG_SIZE, \n",
        "          interpolation='nearest',\n",
        "          label_mode='categorical' # yields \"one-hot\" vectors for output\n",
        "          )\n",
        "\n",
        "  test_set = image_dataset_from_directory('Testset',\n",
        "          shuffle=False,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          image_size=IMG_SIZE, \n",
        "          interpolation='nearest',\n",
        "          label_mode='categorical'\n",
        "          )\n",
        "  class_names = dataset.class_names\n",
        "  class_number = len(class_names)\n",
        "  nbr_batches = tf.data.experimental.cardinality(dataset)\n",
        "  # We take 20% for validation\n",
        "  validation_dataset = dataset.take(nbr_batches // 5)\n",
        "  train_dataset = dataset.skip(nbr_batches // 5)\n",
        "else:\n",
        "  (train_dataset, validation_dataset, test_set), dataset_info = tfds.load(\n",
        "            DATASET_NAME,\n",
        "            split=['train[:70%]', 'train[70%:90%]', 'train[90%:]'],\n",
        "            with_info=True,\n",
        "            as_supervised=True,\n",
        "            shuffle_files= True\n",
        "        )\n",
        "  class_number = dataset_info.features['label'].num_classes\n",
        "  class_names = dataset_info.features['label'].names\n",
        "\n",
        "  # Cast data images to 8 bits and use one hot for labels \n",
        "  def prepare_data(image, label):\n",
        "    image = tf.cast(tf.image.resize(image, IMG_SIZE_TUPLE), tf.uint8)\n",
        "    label = tf.one_hot(label, class_number)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "  train_dataset = train_dataset.map(prepare_data, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.map(prepare_data, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
        "  test_set = test_set.map(prepare_data, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE)"
      ],
      "outputs": [],
      "metadata": {
        "id": "cPcJmjVAKIuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data visualization"
      ],
      "metadata": {
        "id": "SZxvvuLhRght"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype('uint8'))\n",
        "        plt.title(class_names[tf.argmax(labels[i])]) \n",
        "        plt.axis('off')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "11YGseyEPm4s",
        "outputId": "a0bb9a24-8a11-4587-bd1c-4d5090cc46d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augment the data for training set\n",
        "We use `imgaug` library for a fine control on our data augmentation."
      ],
      "metadata": {
        "id": "U2QnkJ-t0faD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sometimes = lambda aug: iaa.Sometimes(0.3, aug)\n",
        "augmenter = iaa.Sequential([\n",
        "    sometimes(iaa.MotionBlur((3, 4))),\n",
        "    sometimes(iaa.GaussianBlur((0.0, 0.75))),\n",
        "    iaa.GammaContrast((0.7, 1.5)),\n",
        "    iaa.MultiplySaturation((0.9, 1.5)),\n",
        "    iaa.MultiplyAndAddToBrightness(),\n",
        "    iaa.Fliplr(p=0.5),\n",
        "    iaa.Affine(scale=(1, 1.3),\n",
        "              translate_percent={\"x\": (-0.3, 0.3), \"y\": (-0.3, 0.3)},\n",
        "              rotate=(-25, 25) )\n",
        "])\n",
        "def augmentation_function(images, labels):\n",
        "    img_dtype = images.dtype\n",
        "    img_shape = tf.shape(images)\n",
        "    images = tf.numpy_function(augmenter.augment_images,\n",
        "                                [images],\n",
        "                                img_dtype)\n",
        "    images = tf.reshape(images, shape=img_shape)\n",
        "    return images, labels\n",
        "\n",
        "train_dataset = train_dataset.map(augmentation_function, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "outputs": [],
      "metadata": {
        "id": "nS5KCOcgRnz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell several times to see the effect on different data.\n"
      ],
      "metadata": {
        "id": "WFS7Wkzh0hxz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Run this cell several times to see the effect on different data\n",
        "for image, _ in train_dataset.unbatch().batch(1).take(1):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "  img_dtype = image.dtype\n",
        "  for i in range(25):\n",
        "    ax = plt.subplot(5, 5, i + 1)\n",
        "    augmented_image = tf.numpy_function(augmenter.augment_images,\n",
        "                                        [image],\n",
        "                                        img_dtype)\n",
        "    plt.imshow(augmented_image[0])\n",
        "    plt.axis('off')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "afP-PgqQwdIA",
        "outputId": "0f0de5c2-3488-4e0b-acc8-73ee5f8785c6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize the data before training"
      ],
      "metadata": {
        "id": "TDkZ5PdZ0qSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "IMG_SHAPE = IMG_SIZE_TUPLE + (3,)\n",
        "if MODEL_VERSION == \"V1\":\n",
        "    normalization = tf.keras.applications.mobilenet.preprocess_input\n",
        "    base_model = tf.keras.applications.MobileNet(input_shape=IMG_SHAPE,\n",
        "                                                  alpha=0.25,\n",
        "                                                  include_top=False,\n",
        "                                                  weights=\"imagenet\")\n",
        "elif MODEL_VERSION == \"V2\":\n",
        "    normalization = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "    base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                                                alpha=0.35,\n",
        "                                                                include_top=False,\n",
        "                                                                weights='imagenet')\n",
        "else:\n",
        "    print('Bad model_version argument, are only accepted : \"V1\", \"V2\"')"
      ],
      "outputs": [],
      "metadata": {
        "id": "SjE4694ER9P-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_dataset = train_dataset.map(lambda img, label: (normalization(tf.cast(img, tf.float32)), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.map(lambda img, label: (normalization(tf.cast(img, tf.float32)), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_set = test_set.map(lambda img, label: (normalization(tf.cast(img, tf.float32)), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fnq50vBYSgSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model instantiation"
      ],
      "metadata": {
        "id": "SSPqdM-_0zet"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "base_model.trainable = False\n",
        "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "outputs = tf.keras.layers.Dense(class_number, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otWvxph_Sjk1",
        "outputId": "32a93892-d003-4e8f-99fd-5440dfcb89cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "tAomoS2b1DgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(train_dataset,\n",
        "                        epochs=100,\n",
        "                        validation_data=validation_dataset, callbacks=callback)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lia3yry7vb9Q",
        "outputId": "8361c154-af42-4b10-d998-4f5c8fe61421"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')"
      ],
      "outputs": [],
      "metadata": {
        "id": "9F-W_kozvi-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model"
      ],
      "metadata": {
        "id": "xaxX-t1Z1IZF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_set = test_set.cache()\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(test_set)\n",
        "print(f\"test loss: {results[0]}, test acc: {results[1]}\")\n",
        "predictions = model.predict(test_set)\n",
        "predictions = tf.argmax(predictions, axis=1)\n",
        "\n",
        "true_categories = tf.argmax( np.concatenate([y for x, y in test_set], axis=0), axis=1)\n",
        "\n",
        "confusion = sk_metrics.confusion_matrix(true_categories, predictions)\n",
        "confusion_normalized = [element/sum(row) for element, row in zip([row for row in confusion], confusion)]\n",
        "axis_labels = list(class_names)\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax = sns.heatmap(confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels, cmap='Blues',\n",
        "                  annot=True,\n",
        "                  fmt='.2f', square=True)\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel(\"True label\")\n",
        "plt.xlabel(\"Predicted label\");"
      ],
      "outputs": [],
      "metadata": {
        "id": "4JNucZ_hv4T9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missclassified samples"
      ],
      "metadata": {
        "id": "49Y2vXQX1NOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "misclassified_images = []\n",
        "misclassified_category = []\n",
        "misclassified_prediction = []\n",
        "for (image, true_category), prediction in zip(test_set.unbatch(), predictions):\n",
        "  true_category = tf.argmax(true_category)\n",
        "  if true_category != prediction:\n",
        "    misclassified_images.append(image.numpy())\n",
        "    misclassified_category.append(true_category)\n",
        "    misclassified_prediction.append(prediction)\n",
        "print('Number of misclassified images : ', len(misclassified_images))"
      ],
      "outputs": [],
      "metadata": {
        "id": "uBGh1RWaxEo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell show a random sample of misclassified images.\n",
        "Run this cell multiple times to view multiples missclassified samples."
      ],
      "metadata": {
        "id": "qhpU5jLA1Uc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(figsize=(17, 13))\n",
        "random_index_shift = randint(0, len(misclassified_images) - 9)\n",
        "for i in range(9):\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  title = 'Actual : ' + class_names[misclassified_category[random_index_shift + i]] + \\\n",
        "          ' Predicted : ' + class_names[misclassified_prediction[random_index_shift + i]]\n",
        "  ax.set_title(title)\n",
        "  plt.imshow((misclassified_images[random_index_shift + i] + 1.0) / 2.0)\n",
        "  plt.axis('off')"
      ],
      "outputs": [],
      "metadata": {
        "id": "ugpLhNkKxG13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantize and export \n",
        "\n",
        "This cell will quantize the model and output a `model_quant.tflite` file that you can use with Cube.AI."
      ],
      "metadata": {
        "id": "8jv82Ppc1mRI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def representative_data_gen():\n",
        "  for x, y in validation_dataset.take(100):\n",
        "    yield [x]\n",
        "\n",
        "# Needed for quantization in case of unfrozen MobileNet model\n",
        "model.trainable = False\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "import pathlib\n",
        "tflite_model_quant = converter.convert()\n",
        "tflite_model_quant_file = pathlib.Path(\"./model_quant.tflite\")\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ],
      "outputs": [],
      "metadata": {
        "id": "b_oFWizWxUrg"
      }
    }
  ]
}